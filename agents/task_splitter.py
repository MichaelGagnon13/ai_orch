from collections import defaultdict

"""
Orchestrateur intelligent qui analyse la complexit√© d'une t√¢che,
la divise en sous-t√¢ches, les ex√©cute via APIs Claude/GPT,
assemble les r√©sultats et valide la coh√©rence finale.
"""

import asyncio
import hashlib
import json
import logging
import os
import time
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

from anthropic import Anthropic
from openai import OpenAI

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class TaskComplexity(Enum):
    """Niveaux de complexit√© des t√¢ches"""

    SIMPLE = "simple"  # < 100 lignes estim√©es
    MODERATE = "moderate"  # 100-300 lignes
    COMPLEX = "complex"  # 300-500 lignes
    VERY_COMPLEX = "very_complex"  # > 500 lignes, n√©cessite division


class AIProvider(Enum):
    """Fournisseurs d'API IA"""

    CLAUDE = "claude"
    GPT = "gpt"
    AUTO = "auto"  # S√©lection automatique


@dataclass
class SubTask:
    """Repr√©sente une sous-t√¢che"""

    id: str
    title: str
    description: str
    estimated_lines: int
    dependencies: List[str]
    priority: int
    context: Optional[Dict[str, Any]] = None
    status: str = "pending"
    result: Optional[str] = None
    execution_time: Optional[float] = None
    provider_used: Optional[str] = None
    error: Optional[str] = None


@dataclass
class TaskAnalysis:
    """Analyse d'une t√¢che"""

    complexity: TaskComplexity
    estimated_total_lines: int
    requires_splitting: bool
    subtasks: List[SubTask]
    execution_strategy: str
    estimated_duration: int  # en minutes


@dataclass
class ExecutionResult:
    """R√©sultat d'ex√©cution d'une t√¢che"""

    success: bool
    final_result: Optional[str]
    subtask_results: List[Dict[str, Any]]
    total_execution_time: float
    validation_score: float
    errors: List[str]
    metadata: Dict[str, Any]


class TaskSplitter:
    """Orchestrateur intelligent de division et ex√©cution de t√¢ches"""

    def __init__(
        self,
        anthropic_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        max_retries: int = 3,
        timeout: int = 300,
        results_dir: str = "results/task_splitter",
    ):
        """
        Initialise l'orchestrateur de t√¢ches.

        Args:
            anthropic_api_key: Cl√© API Anthropic
            openai_api_key: Cl√© API OpenAI
            max_retries: Nombre max de tentatives
            timeout: Timeout par requ√™te
            results_dir: R√©pertoire de sauvegarde
        """
        self.anthropic_key = anthropic_api_key or os.getenv("ANTHROPIC_API_KEY")
        self.openai_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.max_retries = max_retries
        self.timeout = timeout
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Initialisation des clients API
        self.anthropic_client = None
        self.openai_client = None

        if self.anthropic_key:
            try:
                self.anthropic_client = Anthropic(api_key=self.anthropic_key)
                logger.info("Client Anthropic initialis√©")
            except Exception as e:
                logger.error(f"Erreur initialisation Anthropic: {e}")

        if self.openai_key:
            try:
                self.openai_client = OpenAI(api_key=self.openai_key)
                logger.info("Client OpenAI initialis√©")
            except Exception as e:
                logger.error(f"Erreur initialisation OpenAI: {e}")

        if not self.anthropic_client and not self.openai_client:
            raise ValueError("Au moins une cl√© API (Anthropic ou OpenAI) est requise")

        # M√©triques
        self.execution_stats = {
            "tasks_processed": 0,
            "subtasks_created": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "total_execution_time": 0.0,
        }

    async def analyze_task_complexity(
        self, task_description: str, context: Optional[Dict[str, Any]] = None
    ) -> TaskAnalysis:
        """
        Analyse la complexit√© d'une t√¢che et d√©termine s'il faut la diviser.

        Args:
            task_description: Description de la t√¢che
            context: Contexte suppl√©mentaire

        Returns:
            Analyse de la t√¢che
        """
        logger.info("Analyse de la complexit√© de la t√¢che")

        analysis_prompt = f"""
        Analysez cette t√¢che et d√©terminez sa complexit√©:

        T√ÇCHE: {task_description}

        CONTEXTE: {json.dumps(context or {}, indent=2)}

        Fournissez une analyse sous ce format JSON exact:
        {{
            "complexity": "simple|moderate|complex|very_complex",
            "estimated_total_lines": <nombre>,
            "requires_splitting": <boolean>,
            "reasoning": "<explication>",
            "subtasks": [
                {{
                    "title": "<titre>",
                    "description": "<description>",
                    "estimated_lines": <nombre>,
                    "dependencies": ["<id_autre_subtask>"],
                    "priority": <1-10>
                }}
            ],
            "execution_strategy": "<s√©quentiel|parall√®le|hybride>",
            "estimated_duration": <minutes>
        }}

        R√®gles:
        - Une t√¢che > 500 lignes doit √™tre divis√©e
        - Chaque sous-t√¢che < 500 lignes
        - Identifier les d√©pendances entre sous-t√¢ches
        - Prioriser par ordre logique d'ex√©cution
        """

        try:
            response = await self._call_ai_api(analysis_prompt, AIProvider.CLAUDE)
            analysis_data = json.loads(response)

            # Validation des donn√©es
            complexity = TaskComplexity(analysis_data["complexity"])
            requires_splitting = analysis_data["requires_splitting"]

            # Cr√©ation des sous-t√¢ches
            subtasks = []
            for i, subtask_data in enumerate(analysis_data.get("subtasks", [])):
                subtask = SubTask(
                    id=f"subtask_{i+1}",
                    title=subtask_data["title"],
                    description=subtask_data["description"],
                    estimated_lines=subtask_data["estimated_lines"],
                    dependencies=subtask_data.get("dependencies", []),
                    priority=subtask_data.get("priority", 5),
                    context=context,
                )
                subtasks.append(subtask)

            # Si pas de division n√©cessaire, cr√©er une t√¢che unique
            if not requires_splitting and not subtasks:
                subtasks = [
                    SubTask(
                        id="main_task",
                        title="T√¢che principale",
                        description=task_description,
                        estimated_lines=analysis_data["estimated_total_lines"],
                        dependencies=[],
                        priority=1,
                        context=context,
                    )
                ]

            analysis = TaskAnalysis(
                complexity=complexity,
                estimated_total_lines=analysis_data["estimated_total_lines"],
                requires_splitting=requires_splitting,
                subtasks=subtasks,
                execution_strategy=analysis_data.get("execution_strategy", "s√©quentiel"),
                estimated_duration=analysis_data.get("estimated_duration", 30),
            )

            logger.info(f"Analyse termin√©e: {complexity.value}, " f"{len(subtasks)} sous-t√¢ches")

            return analysis

        except Exception as e:
            logger.error(f"Erreur analyse complexit√©: {e}")
            # Analyse de fallback
            return self._create_fallback_analysis(task_description, context)

    def _create_fallback_analysis(
        self, task_description: str, context: Optional[Dict[str, Any]]
    ) -> TaskAnalysis:
        """Cr√©e une analyse de fallback en cas d'erreur"""
        return TaskAnalysis(
            complexity=TaskComplexity.MODERATE,
            estimated_total_lines=300,
            requires_splitting=False,
            subtasks=[
                SubTask(
                    id="fallback_task",
                    title="T√¢che √† traiter",
                    description=task_description,
                    estimated_lines=300,
                    dependencies=[],
                    priority=1,
                    context=context,
                )
            ],
            execution_strategy="s√©quentiel",
            estimated_duration=30,
        )

    async def execute_task(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]] = None,
        preferred_provider: AIProvider = AIProvider.AUTO,
    ) -> ExecutionResult:
        """
        Ex√©cute une t√¢che compl√®te avec division si n√©cessaire.

        Args:
            task_description: Description de la t√¢che
            context: Contexte d'ex√©cution
            preferred_provider: Fournisseur IA pr√©f√©r√©

        Returns:
            R√©sultat d'ex√©cution
        """
        start_time = time.time()
        logger.info(f"D√©but ex√©cution t√¢che: {task_description[:100]}...")

        try:
            # 1. Analyser la complexit√©
            analysis = await self.analyze_task_complexity(task_description, context)

            # 2. Ex√©cuter les sous-t√¢ches
            subtask_results = []
            if analysis.execution_strategy == "s√©quentiel":
                subtask_results = await self._execute_sequential(
                    analysis.subtasks, preferred_provider
                )
            elif analysis.execution_strategy == "parall√®le":
                subtask_results = await self._execute_parallel(
                    analysis.subtasks, preferred_provider
                )
            else:  # hybride
                subtask_results = await self._execute_hybrid(analysis.subtasks, preferred_provider)

            # 3. Assembler les r√©sultats
            final_result = await self._assemble_results(subtask_results, task_description, context)

            # 4. Valider la coh√©rence
            validation_score = await self._validate_coherence(
                final_result, task_description, subtask_results
            )

            # 5. Cr√©er le r√©sultat final
            execution_time = time.time() - start_time
            errors = [r.get("error", "") for r in subtask_results if r.get("error")]

            result = ExecutionResult(
                success=validation_score >= 0.7,
                final_result=final_result,
                subtask_results=subtask_results,
                total_execution_time=execution_time,
                validation_score=validation_score,
                errors=errors,
                metadata={
                    "analysis": asdict(analysis),
                    "execution_strategy": analysis.execution_strategy,
                    "subtasks_count": len(analysis.subtasks),
                    "timestamp": datetime.now().isoformat(),
                },
            )

            # Sauvegarder le r√©sultat
            await self._save_execution_result(result, task_description)

            # Mettre √† jour les stats
            self.execution_stats["tasks_processed"] += 1
            self.execution_stats["subtasks_created"] += len(analysis.subtasks)
            self.execution_stats["total_execution_time"] += execution_time

            if result.success:
                self.execution_stats["successful_executions"] += 1
                logger.info(f"T√¢che ex√©cut√©e avec succ√®s en {execution_time:.2f}s")
            else:
                self.execution_stats["failed_executions"] += 1
                logger.warning(f"T√¢che √©chou√©e, score validation: {validation_score}")

            return result

        except Exception as e:
            logger.error(f"Erreur ex√©cution t√¢che: {e}")
            execution_time = time.time() - start_time
            self.execution_stats["failed_executions"] += 1

            return ExecutionResult(
                success=False,
                final_result=None,
                subtask_results=[],
                total_execution_time=execution_time,
                validation_score=0.0,
                errors=[str(e)],
                metadata={"error": str(e), "timestamp": datetime.now().isoformat()},
            )

    async def _execute_sequential(
        self, subtasks: List[SubTask], preferred_provider: AIProvider
    ) -> List[Dict[str, Any]]:
        """Ex√©cute les sous-t√¢ches s√©quentiellement"""
        logger.info(f"Ex√©cution s√©quentielle de {len(subtasks)} sous-t√¢ches")

        results = []
        completed_tasks = {}

        # Trier par priorit√© et d√©pendances
        sorted_subtasks = self._sort_subtasks_by_dependencies(subtasks)

        for subtask in sorted_subtasks:
            logger.info(f"Ex√©cution sous-t√¢che: {subtask.title}")

            try:
                # V√©rifier les d√©pendances
                if not self._dependencies_satisfied(subtask, completed_tasks):
                    raise ValueError(f"D√©pendances non satisfaites pour {subtask.id}")

                # Construire le contexte avec les r√©sultats des d√©pendances
                execution_context = self._build_execution_context(subtask, completed_tasks)

                # Ex√©cuter la sous-t√¢che
                result = await self._execute_subtask(subtask, execution_context, preferred_provider)
                results.append(result)

                if result["success"]:
                    completed_tasks[subtask.id] = result
                    subtask.status = "completed"
                    subtask.result = result["result"]
                else:
                    subtask.status = "failed"
                    subtask.error = result.get("error")

            except Exception as e:
                logger.error(f"Erreur sous-t√¢che {subtask.id}: {e}")
                result = {
                    "subtask_id": subtask.id,
                    "success": False,
                    "error": str(e),
                    "execution_time": 0.0,
                }
                results.append(result)
                subtask.status = "failed"
                subtask.error = str(e)

        return results

    async def _execute_parallel(
        self, subtasks: List[SubTask], preferred_provider: AIProvider
    ) -> List[Dict[str, Any]]:
        """Ex√©cute les sous-t√¢ches en parall√®le (sans d√©pendances)"""
        logger.info(f"Ex√©cution parall√®le de {len(subtasks)} sous-t√¢ches")

        # Filtrer les t√¢ches sans d√©pendances pour le parall√©lisme
        independent_tasks = [t for t in subtasks if not t.dependencies]
        dependent_tasks = [t for t in subtasks if t.dependencies]

        results = []

        # Ex√©cuter les t√¢ches ind√©pendantes en parall√®le
        if independent_tasks:
            parallel_results = await asyncio.gather(
                *[
                    self._execute_subtask(task, {}, preferred_provider)
                    for task in independent_tasks
                ],
                return_exceptions=True,
            )

            for i, result in enumerate(parallel_results):
                if isinstance(result, Exception):
                    result = {
                        "subtask_id": independent_tasks[i].id,
                        "success": False,
                        "error": str(result),
                        "execution_time": 0.0,
                    }
                results.append(result)

        # Ex√©cuter les t√¢ches d√©pendantes s√©quentiellement
        if dependent_tasks:
            sequential_results = await self._execute_sequential(dependent_tasks, preferred_provider)
            results.extend(sequential_results)

        return results

    async def _execute_hybrid(
        self, subtasks: List[SubTask], preferred_provider: AIProvider
    ) -> List[Dict[str, Any]]:
        """Ex√©cute avec strat√©gie hybride (parall√®le + s√©quentiel)"""
        logger.info(f"Ex√©cution hybride de {len(subtasks)} sous-t√¢ches")

        # Analyser le graphe de d√©pendances
        dependency_levels = self._analyze_dependency_levels(subtasks)
        results = []
        completed_tasks = {}

        # Ex√©cuter par niveaux de d√©pendances
        for level, level_tasks in dependency_levels.items():
            logger.info(f"Ex√©cution niveau {level}: {len(level_tasks)} t√¢ches")

            if len(level_tasks) == 1:
                # Ex√©cution s√©quentielle pour une seule t√¢che
                task = level_tasks[0]
                context = self._build_execution_context(task, completed_tasks)
                result = await self._execute_subtask(task, context, preferred_provider)
                results.append(result)

                if result["success"]:
                    completed_tasks[task.id] = result
            else:
                # Ex√©cution parall√®le pour plusieurs t√¢ches
                level_results = await asyncio.gather(
                    *[
                        self._execute_subtask(
                            task,
                            self._build_execution_context(task, completed_tasks),
                            preferred_provider,
                        )
                        for task in level_tasks
                    ],
                    return_exceptions=True,
                )

                for i, result in enumerate(level_results):
                    if isinstance(result, Exception):
                        result = {
                            "subtask_id": level_tasks[i].id,
                            "success": False,
                            "error": str(result),
                            "execution_time": 0.0,
                        }
                    results.append(result)

                    if result["success"]:
                        completed_tasks[level_tasks[i].id] = result

        return results

    async def _execute_subtask(
        self, subtask: SubTask, context: Dict[str, Any], preferred_provider: AIProvider
    ) -> Dict[str, Any]:
        """
        Ex√©cute une sous-t√¢che individuelle.

        Args:
            subtask: Sous-t√¢che √† ex√©cuter
            context: Contexte d'ex√©cution
            preferred_provider: Fournisseur pr√©f√©r√©

        Returns:
            R√©sultat de l'ex√©cution
        """
        start_time = time.time()

        execution_prompt = f"""
        Ex√©cutez cette sous-t√¢che avec pr√©cision:

        TITRE: {subtask.title}
        DESCRIPTION: {subtask.description}
        LIGNES ESTIM√âES: {subtask.estimated_lines}

        CONTEXTE: {json.dumps(context, indent=2)}

        INSTRUCTIONS:
        - Fournissez un r√©sultat complet et fonctionnel
        - Respectez la limite de ~{subtask.estimated_lines} lignes
        - Utilisez le contexte fourni si pertinent
        - Formatez proprement le code/r√©sultat
        - Ajoutez des commentaires explicatifs

        R√âSULTAT:
        """

        try:
            # S√©lectionner le provider
            provider = self._select_provider(preferred_provider, subtask)

            # Ex√©cuter avec retry
            result_text = None
            for attempt in range(self.max_retries):
                try:
                    result_text = await self._call_ai_api(execution_prompt, provider)
                    break
                except Exception as e:
                    if attempt == self.max_retries - 1:
                        raise e
                    logger.warning(f"Tentative {attempt + 1} √©chou√©e: {e}")
                    await asyncio.sleep(2**attempt)  # Backoff exponentiel

            execution_time = time.time() - start_time

            # Valider le r√©sultat
            is_valid = await self._validate_subtask_result(subtask, result_text)

            return {
                "subtask_id": subtask.id,
                "success": is_valid,
                "result": result_text,
                "execution_time": execution_time,
                "provider_used": provider.value,
                "lines_count": len(result_text.split("\n")) if result_text else 0,
            }

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Erreur ex√©cution sous-t√¢che {subtask.id}: {e}")

            return {
                "subtask_id": subtask.id,
                "success": False,
                "error": str(e),
                "execution_time": execution_time,
            }

    async def _assemble_results(
        self,
        subtask_results: List[Dict[str, Any]],
        original_task: str,
        context: Optional[Dict[str, Any]],
    ) -> str:
        """
        Assemble les r√©sultats des sous-t√¢ches en un r√©sultat final coh√©rent.

        Args:
            subtask_results: R√©sultats des sous-t√¢ches
            original_task: T√¢che originale
            context: Contexte

        Returns:
            R√©sultat final assembl√©
        """
        logger.info("Assemblage des r√©sultats")

        # Filtrer les r√©sultats r√©ussis
        successful_results = [r for r in subtask_results if r.get("success", False)]

        if not successful_results:
            return "ERREUR: Aucune sous-t√¢che n'a r√©ussi"

        # Construire le prompt d'assemblage
        results_text = ""
        for i, result in enumerate(successful_results):
            results_text += (
                f"\n--- R√âSULTAT SOUS-T√ÇCHE {i+1} (ID: {result.get('subtask_id')}) ---\n"
            )
            results_text += result.get("result", "")
            results_text += "\n"

        assembly_prompt = f"""
        Assemblez ces r√©sultats de sous-t√¢ches en un r√©sultat final coh√©rent:

        T√ÇCHE ORIGINALE: {original_task}
        CONTEXTE: {json.dumps(context or {}, indent=2)}

        R√âSULTATS √Ä ASSEMBLER:
        {results_text}

        INSTRUCTIONS D'ASSEMBLAGE:
        - Cr√©ez un r√©sultat final unifi√© et coh√©rent
        - Respectez l'intention de la t√¢che originale
        - Int√©grez harmonieusement tous les √©l√©ments r√©ussis
        - R√©solvez les √©ventuelles contradictions
        - Ajoutez des transitions si n√©cessaire
        - Maintenez la qualit√© et la lisibilit√©

        R√âSULTAT FINAL ASSEMBL√â:
        """

        try:
            final_result = await self._call_ai_api(assembly_prompt, AIProvider.CLAUDE)
            logger.info("Assemblage termin√© avec succ√®s")
            return final_result

        except Exception as e:
            logger.error(f"Erreur assemblage: {e}")
            # Fallback: concat√©nation simple
            return "\n\n".join([r.get("result", "") for r in successful_results])

    async def _validate_coherence(
        self, final_result: str, original_task: str, subtask_results: List[Dict[str, Any]]
    ) -> float:
        """
        Valide la coh√©rence du r√©sultat final.

        Args:
            final_result: R√©sultat final
            original_task: T√¢che originale
            subtask_results: R√©sultats des sous-t√¢ches

        Returns:
            Score de validation (0.0 √† 1.0)
        """
        logger.info("Validation de la coh√©rence")

        validation_prompt = f"""
        √âvaluez la coh√©rence de ce r√©sultat final par rapport √† la t√¢che originale:

        T√ÇCHE ORIGINALE: {original_task}

        R√âSULTAT FINAL:
        {final_result}

        CRIT√àRES D'√âVALUATION:
        1. Compl√©tude: Le r√©sultat r√©pond-il enti√®rement √† la t√¢che? (0-25 points)
        2. Coh√©rence: Le r√©sultat est-il logique et coh√©rent? (0-25 points)
        3. Qualit√©: Le code/contenu est-il de bonne qualit√©? (0-25 points)
        4. Fonctionnalit√©: Le r√©sultat est-il utilisable/fonctionnel? (0-25 points)

        R√©pondez UNIQUEMENT avec un JSON:
        {{
            "completude": <0-25>,
            "coherence": <0-25>,
            "qualite": <0-25>,
            "fonctionnalite": <0-25>,
            "score_total": <0-100>,
            "commentaires": "<explication courte>"
        }}
        """

        try:
            response = await self._call_ai_api(validation_prompt, AIProvider.GPT)
            validation_data = json.loads(response)

            score = validation_data.get("score_total", 0) / 100.0
            logger.info(f"Score de validation: {score:.2f}")

            return max(0.0, min(1.0, score))

        except Exception as e:
            logger.error(f"Erreur validation: {e}")
            # Score de fallback bas√© sur des heuristiques simples
            if not final_result or len(final_result.strip()) < 50:
                return 0.2

            successful_subtasks = len([r for r in subtask_results if r.get("success")])
            total_subtasks = len(subtask_results)

            if total_subtasks == 0:
                return 0.5

            return min(0.8, successful_subtasks / total_subtasks)

    async def _call_ai_api(self, prompt: str, provider: AIProvider) -> str:
        """
        Appelle l'API IA appropri√©e.

        Args:
            prompt: Prompt √† envoyer
            provider: Fournisseur √† utiliser

        Returns:
            R√©ponse de l'API
        """
        if provider == AIProvider.AUTO:
            provider = self._select_best_provider()

        try:
            if provider == AIProvider.CLAUDE and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=32000,
                    messages=[{"role": "user", "content": prompt}],
                )
                return response.content[0].text

            elif provider == AIProvider.GPT and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-5.1",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=32000,
                )
                return response.choices[0].message.content

            else:
                raise ValueError(f"Provider {provider} non disponible")

        except Exception as e:
            logger.error(f"Erreur API {provider}: {e}")
            raise

    def _select_provider(self, preferred: AIProvider, subtask: SubTask) -> AIProvider:
        """S√©lectionne le meilleur provider pour une sous-t√¢che"""
        if preferred != AIProvider.AUTO:
            return preferred

        # Logique de s√©lection bas√©e sur le type de t√¢che
        if "code" in subtask.description.lower() or "python" in subtask.description.lower():
            return AIProvider.GPT if self.openai_client else AIProvider.CLAUDE
        else:
            return AIProvider.CLAUDE if self.anthropic_client else AIProvider.GPT

    def _select_best_provider(self) -> AIProvider:
        """S√©lectionne le meilleur provider disponible"""
        if self.anthropic_client:
            return AIProvider.CLAUDE
        elif self.openai_client:
            return AIProvider.GPT
        else:
            raise ValueError("Aucun provider disponible")

    def _sort_subtasks_by_dependencies(self, subtasks: List[SubTask]) -> List[SubTask]:
        """Trie les sous-t√¢ches par ordre de d√©pendances"""
        sorted_tasks = []
        remaining_tasks = subtasks.copy()
        completed_ids = set()

        while remaining_tasks:
            # Trouver les t√¢ches sans d√©pendances non satisfaites
            ready_tasks = []
            for task in remaining_tasks:
                if all(dep in completed_ids for dep in task.dependencies):
                    ready_tasks.append(task)

            if not ready_tasks:
                # D√©pendances circulaires d√©tect√©es, prendre la premi√®re
                ready_tasks = [remaining_tasks[0]]
                logger.warning("D√©pendances circulaires d√©tect√©es")

            # Trier par priorit√©
            ready_tasks.sort(key=lambda t: t.priority)

            for task in ready_tasks:
                sorted_tasks.append(task)
                completed_ids.add(task.id)
                remaining_tasks.remove(task)

        return sorted_tasks

    def _dependencies_satisfied(self, subtask: SubTask, completed_tasks: Dict[str, Any]) -> bool:
        """V√©rifie si les d√©pendances d'une sous-t√¢che sont satisfaites"""
        return all(dep in completed_tasks for dep in subtask.dependencies)

    def _build_execution_context(
        self, subtask: SubTask, completed_tasks: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Construit le contexte d'ex√©cution avec les r√©sultats des d√©pendances"""
        context = subtask.context.copy() if subtask.context else {}

        # Ajouter les r√©sultats des d√©pendances
        dependencies_results = {}
        for dep_id in subtask.dependencies:
            if dep_id in completed_tasks:
                dependencies_results[dep_id] = completed_tasks[dep_id].get("result", "")

        if dependencies_results:
            context["dependencies_results"] = dependencies_results

        return context

    def _analyze_dependency_levels(self, subtasks: List[SubTask]) -> Dict[int, List[SubTask]]:
        """Analyse les niveaux de d√©pendances pour l'ex√©cution hybride"""
        levels = defaultdict(list)
        task_levels = {}

        # Calculer le niveau de chaque t√¢che
        for task in subtasks:
            level = self._calculate_task_level(task, subtasks, task_levels)
            levels[level].append(task)
            task_levels[task.id] = level

        return dict(levels)

    def _calculate_task_level(
        self, task: SubTask, all_tasks: List[SubTask], task_levels: Dict[str, int]
    ) -> int:
        """Calcule le niveau de d√©pendance d'une t√¢che"""
        if task.id in task_levels:
            return task_levels[task.id]

        if not task.dependencies:
            task_levels[task.id] = 0
            return 0

        # Trouver le niveau maximum des d√©pendances
        max_dep_level = -1
        for dep_id in task.dependencies:
            dep_task = next((t for t in all_tasks if t.id == dep_id), None)
            if dep_task:
                dep_level = self._calculate_task_level(dep_task, all_tasks, task_levels)
                max_dep_level = max(max_dep_level, dep_level)

        level = max_dep_level + 1
        task_levels[task.id] = level
        return level

    async def _validate_subtask_result(self, subtask: SubTask, result: str) -> bool:
        """Valide le r√©sultat d'une sous-t√¢che"""
        if not result or len(result.strip()) < 10:
            return False

        # Validation basique
        if subtask.estimated_lines > 0:
            result_lines = len(result.split("\n"))
            # Tol√©rance de ¬±50% sur l'estimation
            min_lines = max(10, subtask.estimated_lines * 0.5)
            max_lines = subtask.estimated_lines * 2

            if not (min_lines <= result_lines <= max_lines):
                logger.warning(
                    f"Taille r√©sultat hors estimation: "
                    f"{result_lines} lignes vs {subtask.estimated_lines} attendues"
                )

        return True

    async def _save_execution_result(self, result: ExecutionResult, task_description: str):
        """Sauvegarde le r√©sultat d'ex√©cution"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_hash = hashlib.md5(task_description.encode()).hexdigest()[:8]
            filename = f"task_result_{timestamp}_{task_hash}.json"
            filepath = self.results_dir / filename

            # Pr√©parer les donn√©es √† sauvegarder
            save_data = {
                "task_description": task_description,
                "timestamp": datetime.now().isoformat(),
                "result": asdict(result),
                "execution_stats": self.execution_stats.copy(),
            }

            # Sauvegarder en JSON
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)

            # Sauvegarder aussi le r√©sultat final en texte
            if result.final_result:
                text_filename = f"task_result_{timestamp}_{task_hash}.txt"
                text_filepath = self.results_dir / text_filename

                with open(text_filepath, "w", encoding="utf-8") as f:
                    f.write(f"T√ÇCHE: {task_description}\n")
                    f.write(f"TIMESTAMP: {datetime.now().isoformat()}\n")
                    f.write(f"SUCC√àS: {result.success}\n")
                    f.write(f"SCORE VALIDATION: {result.validation_score:.2f}\n")
                    f.write("\n" + "=" * 50 + "\n")
                    f.write("R√âSULTAT FINAL:\n")
                    f.write("=" * 50 + "\n\n")
                    f.write(result.final_result)

            logger.info(f"R√©sultat sauvegard√©: {filepath}")

        except Exception as e:
            logger.error(f"Erreur sauvegarde: {e}")

    def get_execution_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques d'ex√©cution"""
        stats = self.execution_stats.copy()
        if stats["tasks_processed"] > 0:
            stats["average_execution_time"] = (
                stats["total_execution_time"] / stats["tasks_processed"]
            )
            stats["success_rate"] = stats["successful_executions"] / stats["tasks_processed"]
        else:
            stats["average_execution_time"] = 0.0
            stats["success_rate"] = 0.0

        return stats


async def main():
    """Fonction de test"""
    # Configuration
    splitter = TaskSplitter()

    # T√¢che de test
    test_task = """
    Cr√©er un syst√®me de gestion de biblioth√®que en Python avec:
    1. Classes pour Livre, Auteur, Emprunteur
    2. Base de donn√©es SQLite pour la persistance
    3. API REST avec FastAPI
    4. Interface web simple avec HTML/CSS/JS
    5. Tests unitaires avec pytest
    6. Documentation compl√®te
    """

    print("üöÄ Test du TaskSplitter")
    print(f"T√¢che: {test_task}")

    # Ex√©cuter la t√¢che
    result = await splitter.execute_task(test_task)

    print("\n‚úÖ R√©sultat:")
    print(f"Succ√®s: {result.success}")
    print(f"Temps d'ex√©cution: {result.total_execution_time:.2f}s")
    print(f"Score validation: {result.validation_score:.2f}")
    print(f"Sous-t√¢ches: {len(result.subtask_results)}")

    if result.errors:
        print(f"Erreurs: {result.errors}")

    # Statistiques
    stats = splitter.get_execution_stats()
    print(f"\nüìä Statistiques: {stats}")


if __name__ == "__main__":
    asyncio.run(main())
